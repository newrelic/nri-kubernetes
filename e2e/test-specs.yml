description: |
  End-to-end tests for k8s integration

custom_test_key: k8s.clusterName

scenarios:
  - description: |
      This scenario will verify that metrics from a k8s Cluster are correctly collected.
    before:
      - helm dependency update ../charts/e2e-resources
      - helm dependency update ../charts/newrelic-infrastructure
      - helm upgrade --install ${SCENARIO_TAG}-resources -n nr-${SCENARIO_TAG} --create-namespace ../charts/e2e-resources --set persistentVolume.enabled=true
      - helm upgrade --install ${SCENARIO_TAG} -n nr-${SCENARIO_TAG} --create-namespace ../charts/newrelic-infrastructure --values e2e-values.yml --set global.licenseKey=${LICENSE_KEY} --set global.cluster=${SCENARIO_TAG}
    after:
      - kubectl logs -l app.kubernetes.io/name=newrelic-infrastructure-v3 -n nr-${SCENARIO_TAG} --all-containers --prefix=true
      - kubectl get pods -n nr-${SCENARIO_TAG}
      - helm delete ${SCENARIO_TAG}-resources -n nr-${SCENARIO_TAG}
      - helm delete ${SCENARIO_TAG} -n nr-${SCENARIO_TAG}
    tests:
      nrqls: []
      entities: []
      metrics:
        - source: "k8s.yml"
          except_entities: []
          except_metrics:
            - k8s.node.allocatableHugepages*
            - k8s.node.capacity*
            - k8s.node.capacityAttachableVolumes*
            - k8s.node.allocatableAttachableVolumes*

            - k8s.controllermanager.leaderElectionMasterStatus

            - k8s.scheduler.leaderElectionMasterStatus
            - k8s.scheduler.podPreemptionVictims
            - k8s.scheduler.preemptionAttemptsDelta
            - k8s.scheduler.schedulingDurationSeconds_*

            # Network metrics are flaky and sometimes fail
            - k8s.pod.netErrorsPerSecond
            - k8s.pod.netRxBytesPerSecond
            - k8s.pod.netTxBytesPerSecond

  - description: |
      This scenario will verify that metrics from a k8s Cluster are correctly collected without privileges.
    before:
      - helm dependency update ../charts/e2e-resources
      - helm dependency update ../charts/newrelic-infrastructure
      - helm upgrade --install ${SCENARIO_TAG}-resources -n nr-${SCENARIO_TAG} --create-namespace ../charts/e2e-resources --set persistentVolume.enabled=true
      - helm upgrade --install ${SCENARIO_TAG} -n nr-${SCENARIO_TAG} --create-namespace ../charts/newrelic-infrastructure --values e2e-values.yml --set global.licenseKey=${LICENSE_KEY} --set global.cluster=${SCENARIO_TAG} --set privileged=false
    after:
      - kubectl logs -l app.kubernetes.io/name=newrelic-infrastructure-v3 -n nr-${SCENARIO_TAG} --all-containers --prefix=true
      - kubectl get pods -n nr-${SCENARIO_TAG}
      - helm delete ${SCENARIO_TAG}-resources -n nr-${SCENARIO_TAG}
      - helm delete ${SCENARIO_TAG} -n nr-${SCENARIO_TAG}
    tests:
      nrqls: []
      entities: []
      metrics:
        - source: "k8s.yml"
          except_entities:
            - K8sCluster # all metrics are related to controlPlane
          except_metrics:
            - k8s.node.allocatableHugepages*
            - k8s.node.capacity*
            - k8s.node.capacityAttachableVolumes*
            - k8s.node.allocatableAttachableVolumes*
            
            # Network metrics are flaky and sometimes fail
            - k8s.pod.netErrorsPerSecond
            - k8s.pod.netRxBytesPerSecond
            - k8s.pod.netTxBytesPerSecond
