description: |
  End-to-end tests for k8s integration

custom_test_key: k8s.clusterName

scenarios:
  - description: |
      This scenario will verify that metrics from a k8s Cluster are correctly collected.
    before:
      - helm dependency update ../charts/internal/e2e-resources
      - helm dependency update ../charts/newrelic-infrastructure
      - function ver { printf $((10#$(printf "%03d%03d" $(echo "$1" | tr '.' ' ')))); } && K8S_VERSION=$(kubectl version 2>&1 | grep 'Server Version' | awk -F' v' '{ print $2; }' | awk -F. '{ print $1"."$2; }') && if [[ $(ver $K8S_VERSION) -gt $(ver "1.22") ]]; then KSM_IMAGE_VERSION="v2.10.0"; else KSM_IMAGE_VERSION="v2.6.0"; fi && echo "Will use KSM image version ${KSM_IMAGE_VERSION}" && helm upgrade --install ${SCENARIO_TAG}-resources -n nr-${SCENARIO_TAG} --create-namespace ../charts/internal/e2e-resources --set persistentVolume.enabled=true --set kube-state-metrics.image.tag=${KSM_IMAGE_VERSION}
      - helm upgrade --install ${SCENARIO_TAG} -n nr-${SCENARIO_TAG} --create-namespace ../charts/newrelic-infrastructure --values e2e-values.yml --set global.licenseKey=${LICENSE_KEY} --set global.cluster=${SCENARIO_TAG}
    after:
      - kubectl logs -l app.kubernetes.io/name=newrelic-infrastructure -n nr-${SCENARIO_TAG} --all-containers --prefix=true
      - kubectl get pods -n nr-${SCENARIO_TAG}
      - helm delete ${SCENARIO_TAG}-resources -n nr-${SCENARIO_TAG}
      - helm delete ${SCENARIO_TAG} -n nr-${SCENARIO_TAG}
    tests:
      nrqls: []
      entities: []
      metrics:
        - source: "k8s.yml"
          # EXCEPTIONS_SOURCE_FILE contains the path to the exceptions according to the k8s version.
          # These exceptions files live in '/e2e' path and are selected on the GH e2e workflow.
          exceptions_source: ${EXCEPTIONS_SOURCE_FILE}
          except_entities: []
          except_metrics:
            - k8s.node.allocatableHugepages*
            - k8s.node.capacity*
            - k8s.node.capacityAttachableVolumes*
            - k8s.node.allocatableAttachableVolumes*

            - k8s.controllermanager.leaderElectionMasterStatus

            - k8s.scheduler.leaderElectionMasterStatus
            - k8s.scheduler.podPreemptionVictims
            - k8s.scheduler.preemptionAttemptsDelta
            - k8s.scheduler.schedulingDurationSeconds_*

            # Network metrics are flaky and sometimes fail
            - k8s.pod.netErrorsPerSecond
            - k8s.pod.netRxBytesPerSecond
            - k8s.pod.netTxBytesPerSecond

            # this metric does not appear when the scaler is not limited.
            - k8s.hpa.isLimited

            # Labels are attributes, not metrics. This erroneous metric has been removed
            - k8s.hpa.labels

            # etcd_object_counts was deprecated in k8s 1.22 and removed in 1.23 (replaced by apiserver_storage_objects)
            - k8s.apiserver.etcd.objectCount_*

  - description: |
      This scenario will verify that metrics from a k8s Cluster are correctly collected without privileges.
    before:
      - helm dependency update ../charts/internal/e2e-resources
      - helm dependency update ../charts/newrelic-infrastructure
      - function ver { printf $((10#$(printf "%03d%03d" $(echo "$1" | tr '.' ' ')))); } && K8S_VERSION=$(kubectl version 2>&1 | grep 'Server Version' | awk -F' v' '{ print $2; }' | awk -F. '{ print $1"."$2; }') && if [[ $(ver $K8S_VERSION) -gt $(ver "1.22") ]]; then KSM_IMAGE_VERSION="v2.10.0"; else KSM_IMAGE_VERSION="v2.6.0"; fi && echo "Will use KSM image version ${KSM_IMAGE_VERSION}" && helm upgrade --install ${SCENARIO_TAG}-resources -n nr-${SCENARIO_TAG} --create-namespace ../charts/internal/e2e-resources --set persistentVolume.enabled=true --set kube-state-metrics.image.tag=${KSM_IMAGE_VERSION} 
      - helm upgrade --install ${SCENARIO_TAG} -n nr-${SCENARIO_TAG} --create-namespace ../charts/newrelic-infrastructure --values e2e-values.yml --set global.licenseKey=${LICENSE_KEY} --set global.cluster=${SCENARIO_TAG} --set privileged=false
    after:
      - kubectl logs -l app.kubernetes.io/name=newrelic-infrastructure -n nr-${SCENARIO_TAG} --all-containers --prefix=true
      - kubectl get pods -n nr-${SCENARIO_TAG}
      - helm delete ${SCENARIO_TAG}-resources -n nr-${SCENARIO_TAG}
      - helm delete ${SCENARIO_TAG} -n nr-${SCENARIO_TAG}
    tests:
      nrqls: []
      entities: []
      metrics:
        - source: "k8s.yml"
          exceptions_source: ${EXCEPTIONS_SOURCE_FILE}
          except_entities:
            - K8sCluster # all metrics are related to controlPlane
          except_metrics:
            - k8s.node.allocatableHugepages*
            - k8s.node.capacity*
            - k8s.node.capacityAttachableVolumes*
            - k8s.node.allocatableAttachableVolumes*
            
            # Network metrics are flaky and sometimes fail
            - k8s.pod.netErrorsPerSecond
            - k8s.pod.netRxBytesPerSecond
            - k8s.pod.netTxBytesPerSecond

            - k8s.hpa.isLimited

            # Labels are attributes, not metrics. This erroneous metric has been removed
            - k8s.hpa.labels
